{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "![stardist](../../../images/deblurring.jpeg)\n",
    "\n",
    "# Total-Variation based Bayesian Image Deblurring\n",
    "\n",
    "In this notebook, we will perform image deblurring via a Bayesian *Maximum a Posteriori* (MAP) approach. Our image prior will be composed of a total variation (TV) term and a positivity constraint.\n",
    "\n",
    "```{admonition} Acknowledgements\n",
    "This notebook is part of the offical [Pyxu Example Gallery](https://pyxu-org.github.io/examples/index.html). We kindly acknowledge them for sharing their tutorial with us!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "\n",
    "The forward model defining the blurring process is defined as:\n",
    "\n",
    "$$\\mathbf{y}=\\mathbf{H}\\mathbf{x}+\\mathbf{n}$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\mathbf{y} \\in \\mathbb{R}^{d}$ is the observed blurred and noisy image,\n",
    "- $\\mathbf{H}: \\mathbb{R}^{d\\times d}$ is the blurring operator, which consists on a convolution with a Gaussian point-spread-function (PSF),\n",
    "- $\\mathbf{x} \\in \\mathbb{R}^{d}$ is the original clean image we want to recover,\n",
    "- $\\mathbf{n} \\in \\mathbb{R}^{d}$ is independent and identically distributed Gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries and modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "from pyxu.operator import Convolve, L21Norm, Gradient, SquaredL2Norm, PositiveOrthant\n",
    "from pyxu.opt.solver import PD3O\n",
    "from pyxu.opt.stop import RelError, MaxIter\n",
    "\n",
    "# Setting up GPU support\n",
    "GPU = False\n",
    "if GPU:\n",
    "    import cupy as xp\n",
    "else:\n",
    "    import numpy as xp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing the Image\n",
    "\n",
    "We will use a sample image from the `skimage.data` module and preprocess it to be suitable for the deblurring process. The image is converted to a float type and normalized to have pixel values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and preprocessing the image\n",
    "data = skimage.data.coffee()\n",
    "skimage.io.imshow(data)\n",
    "data = xp.asarray(data.astype(\"float32\") / 255.0).transpose(2, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Creating the Blurring Kernel\n",
    "\n",
    "We will create a Gaussian blurring kernel (a.k.a. point spread function or PSF) to simulate the blurring effect of the camera lens on the image. The kernel is defined by its standard deviation and width. The Gaussian function is given by:\n",
    "$$ G(x)=\\frac{1}{2\\pi\\sigma^{2}} e^{âˆ’\\frac{(xâˆ’\\mu)^{2}}{2\\sigma^{2}}} $$\n",
    "\n",
    "where:\n",
    "\n",
    "- $G(x)$ is the Gaussian function,\n",
    "- $\\sigma$ is the standard deviation,\n",
    "- $\\mu$ is the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Gaussian blurring kernel\n",
    "sigma = 7\n",
    "width = 13\n",
    "mu = (width - 1) / 2\n",
    "gauss = lambda x: (1 / (2 * np.pi * sigma**2)) * np.exp(\n",
    "    -0.5 * ((x - mu) ** 2) / (sigma**2)\n",
    ")\n",
    "\n",
    "kernel_1d = np.fromfunction(gauss, (width,)).reshape(1, -1)\n",
    "kernel_1d /= kernel_1d.sum()\n",
    "\n",
    "kernel_1d = xp.asarray(kernel_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Applying the Blurring and Adding Noise\n",
    "\n",
    "We will use the created Gaussian kernel to blur the image and then add Gaussian noise to simulate a real-world scenario where camera sensors are corrupted by thermal noise. Note that the 2D Gaussian kernel is defined in a separable fashion for efficiency reasons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the blurring and adding noise\n",
    "conv = Convolve(\n",
    "    arg_shape=data.shape,\n",
    "    kernel=[xp.array([1]), kernel_1d, kernel_1d], \n",
    "    center=[0, width // 2, width // 2],\n",
    "    mode=\"reflect\",\n",
    "    enable_warnings=True,\n",
    ")\n",
    "y = conv(data.ravel()).reshape(data.shape)\n",
    "y = xp.random.normal(loc=y, scale=0.05)\n",
    "y = y.clip(0, 1)\n",
    "skimage.io.imshow(y.transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## MAP Estimate with Composite Positivity + Total Variation Prior \n",
    "\n",
    "Maximum a Posteriori seeks the most credible output given the likelihood and image prior, that is a mode of posterior distribution (not necessarily unique, but globally optimal in the log-concave case). The likelihood model is based on the noise distribution (here assumed Gaussian), and the prior model incorporates our assumptions about the image. The total variation prior promotes \"mostly flat\" solutions, helping to preserve edges while smoothing out noise. The positivity constraint ensures that the pixel values of the deblurred image remain non-negative.\n",
    "\n",
    "The MAP optimization problem can be written as:\n",
    "$$\\hat{\\mathbf{x}}=\\arg\\min_{\\mathbf{x} â‰¥0} \\frac{1}{2}\\Vert \\mathbf{y}âˆ’ \\mathbf{H}\\mathbf{x}\\Vert^{2}_{2}+\\lambda\\Vert\\nabla\\mathbf{x}\\Vert_{1,2}$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\Vert\\mathbf{y}âˆ’\\mathbf{H}\\mathbf{x}\\Vert_2^2$ is the squared $L_2$-norm representing the data fidelity term (likelihood),\n",
    "- $\\Vert \\nabla \\mathbf{x}\\Vert_{2, 1}=\\sqrt{\\Vert \\nabla_{x} \\mathbf{x} \\Vert_{1}^{2} + \\Vert \\nabla_{y} \\mathbf{x}\\Vert_{1}^{2}}$ is the isotropic total variation norm,\n",
    "- $\\lambda$ is the regularization parameter,\n",
    "- $\\mathbf{x}â‰¥0$ is the positivity constraint.\n",
    "\n",
    "We solve this problem with the PD3O solver [ðŸ”—](../api/opt.solver.html#pyxu.opt.solver.PD3O), with\n",
    " \n",
    "- $\\mathcal{F}(\\mathbf{x})=\\frac{1}{2}\\Vert \\mathbf{y}âˆ’ \\mathbf{H}\\mathbf{x}\\Vert^{2}_{2}$,\n",
    "- $\\mathcal{G}(\\mathbf{x})=\\iota_+(\\mathbf{x})$,\n",
    "- $\\mathcal{H}(\\mathbf{z})=\\lambda \\Vert \\mathbf{z}\\Vert_{2, 1}$,\n",
    "- $\\mathcal{K}=\\nabla$.\n",
    "\n",
    "PD3O manages the composite, non-smooth, and non-proximable term $\\mathcal{H}(\\mathcal{K} \\mathbf{x})$ utilizing its Fenchel biconjugate. This implies that, while the minimization of the functionals $\\mathcal{F}$ and $\\mathcal{G}$ occurs on the primal variable of interest, the minimization of the term $\\mathcal{H}(\\mathcal{K} \\mathbf{x})$ is indirectly undertaken on the dual variable, and, upon convergence, subsequently on the primal when the primal-dual gap reduces to zero. Therefore, it is crucial to set the relative improvement convergence threshold of PD3O at a significantly low level to ensure the proper convergence of the algorithm; if not, the resulting solution will not exhibit the mostly flat behavior expected from the application of a Total Variation (TV) prior. A challenge to note is that PD3O, being generically designed, may demonstrate slow convergence when seeking such high accuracies. We can overcome this issue by using the GPU implementation of PD3O, which is significantly faster than the CPU version (e.g., this example runs in approximately 30 seconds in GPU vs. approximately 4 minutes in CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the MAP approach with total variation prior and positivity constraint\n",
    "sl2 = SquaredL2Norm(dim=y.size).asloss(y.ravel())\n",
    "loss = sl2 * conv\n",
    "\n",
    "l21 = L21Norm(arg_shape=(2, *y.shape), l2_axis=(0, 1))\n",
    "\n",
    "grad = Gradient(\n",
    "    arg_shape=y.shape,\n",
    "    directions=(1, 2),\n",
    "    gpu=GPU,\n",
    "    diff_method=\"fd\",\n",
    "    scheme=\"central\",\n",
    "    accuracy=3,\n",
    ")\n",
    "\n",
    "stop_crit = RelError(\n",
    "            eps=1e-6,\n",
    "            var=\"x\",\n",
    "            f=None,\n",
    "            norm=2,\n",
    "            satisfy_all=True,\n",
    "        ) | MaxIter(5000)\n",
    "\n",
    "positivity = PositiveOrthant(dim=y.size)\n",
    "solver = PD3O(f=loss, g=positivity, h= 3e-2 * l21, K=grad, verbosity=500)\n",
    "solver.fit(x0=y.ravel(), stop_crit=stop_crit)\n",
    "\n",
    "# Getting the deblurred image\n",
    "recons = solver.solution().reshape(y.shape)\n",
    "recons /= recons.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "skimage.io.imshow(recons.transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the deblurred image\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import mean_squared_error as mse\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "\n",
    "if GPU:\n",
    "    data = data.get()\n",
    "    y = y.get()\n",
    "    recons = recons.get()\n",
    "\n",
    "data = data.transpose(1, 2, 0)\n",
    "y = y.transpose(1, 2, 0)\n",
    "recons = recons.clip(0,1)\n",
    "recons = recons.transpose(1, 2, 0)\n",
    "\n",
    "mse_y = mse(data, y)\n",
    "ssim_y = ssim(data, y, channel_axis=2, data_range=1.)\n",
    "psnr_y = psnr(data, y, data_range=1.)\n",
    "mse_recons = mse(data, recons)\n",
    "ssim_recons = ssim(data, recons, channel_axis=2, data_range=1.)\n",
    "psnr_recons = psnr(data, recons, data_range=1.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Visualizing the Results\n",
    "\n",
    "Finally, let's visualize the original image, the blurred and noisy image, and the deblurred image obtained using the MAP approach with a total variation prior and positivity constraint. We will also display the evaluation metrics for a comprehensive comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the results\n",
    "\n",
    "plt.figure(figsize=(15, 11))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(data.clip(0,1))\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.imshow(y.clip(0, 1))\n",
    "plt.title(f\"Blurred and Noisy Image\\nMSE: {mse_y:.2f}, SSIM: {ssim_y:.2f}, PSNR: {psnr_y:.2f}\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.imshow(recons)\n",
    "plt.title(f\"Deblurred Image\\nMSE: {mse_recons:.2f}, SSIM: {ssim_recons:.2f}, PSNR: {psnr_recons:.2f}\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
