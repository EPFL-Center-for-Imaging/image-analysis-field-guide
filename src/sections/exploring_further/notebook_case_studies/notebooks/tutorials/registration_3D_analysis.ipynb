{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![geomechanics](../../../../../images/alert-geo.png)\n",
    "# 3D analysis of geomaterials\n",
    "\n",
    "To measure motion between two images of an evolving object, two main principles can be used:\n",
    "\n",
    "  1. **Particle Tracking**: Identification of the object in every time frame, followed by a temporal linking of identified objects. In the fluid mechanics world this is close to \"PTV\" (Particle Tracking Velocimetry). This family of techniques is extremely efficient when reliable object identifications can be made systematically for the material -- it is not only related to granular media: it can be effectively used to track pores, or inclusions in a matrix.\n",
    "  \n",
    "  2. **Image Correlation**: Tracking of greylevel texture using techniques based on greylevel conservation (\"optical flow\") the family of Digital Image Correlation (more correctly Digital Volume Correlation when comparing 3D images), which map into \"PIV\" (Particle Image Velocimetry) in fluid mechanics. This technique is much more general than particle tracking, requiring \"texture\" in both images, but can be applied very successfully to granular materials.\n",
    "\n",
    "In this notebook, we will discuss both methods with some examples, starting with a general introduction for Digital Volume Correlation to introduce a \"global\" measurement of strain which will be used to help a particle tracking approach. Finally image correlation will be also shown at the \"micro\" scale, for in 3D volumes where particles are identifiable, and when they are not.\n",
    "\n",
    "```{admonition} Acknowledgements\n",
    "This notebook was originally part of the 2022 [Alert Geomaterials](https://github.com/alert-geomaterials/2022-doctoral-school) doctoral course. We kindly acknowledge them for sharing their training material with us!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data\n",
    "\n",
    "The datsa we'll use in this tutorial are available for download on [Zenodo](https://zenodo.org/record/7140837/).\n",
    "\n",
    "In the cell below, we use a Python package called [pooch](https://pypi.org/project/pooch/) to automatically download the image from Zenodo into the **data** folder of this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pooch\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = Path(os.path.expanduser(\n",
    "    os.path.join(os.getenv(\"XDG_DATA_HOME\", \"~\"), \".field-guide\")\n",
    "))\n",
    "\n",
    "fname = 'M2EA05-01-bin4.tif'\n",
    "\n",
    "pooch.retrieve(\n",
    "    url=f\"https://zenodo.org/record/7140837/files/{fname}\",\n",
    "    known_hash=\"md5:d8358c14acc5ae65aee67887201c1bb1\",\n",
    "    path=data_path,\n",
    "    fname=fname,\n",
    "    progressbar=True,\n",
    ")\n",
    "\n",
    "print(f'Downloaded image {fname} into: {data_path}')\n",
    "\n",
    "fname = 'M2EA05-05-bin4.tif'\n",
    "\n",
    "pooch.retrieve(\n",
    "    url=f\"https://zenodo.org/record/7140837/files/{fname}\",\n",
    "    known_hash=\"md5:5a2d68d0d8425e9da7118c921d682a5f\",\n",
    "    path=data_path,\n",
    "    fname=fname,\n",
    "    progressbar=True,\n",
    ")\n",
    "\n",
    "print(f'Downloaded image {fname} into: {data_path}')\n",
    "\n",
    "fname = 'M2EA05-01-bin4-lab.tif'\n",
    "\n",
    "pooch.retrieve(\n",
    "    url=f\"https://zenodo.org/record/7140837/files/{fname}\",\n",
    "    known_hash=\"md5:6f4eef96d25ca7f16209e029a26c6828\",\n",
    "    path=data_path,\n",
    "    fname=fname,\n",
    "    progressbar=True,\n",
    ")\n",
    "\n",
    "print(f'Downloaded image {fname} into: {data_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load some example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 546
    },
    "id": "d9YegyvB_iLV",
    "outputId": "0872ae65-8ead-4711-8b17-1d88ff0e3cdd"
   },
   "outputs": [],
   "source": [
    "import tifffile\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "im1 = tifffile.imread(data_path / \"M2EA05-01-bin4.tif\").astype(float)\n",
    "print(f\"Shape of im1 (Z,Y,X): {im1.shape}\")\n",
    "im5 = tifffile.imread(data_path / \"M2EA05-05-bin4.tif\").astype(float)\n",
    "\n",
    "# Let's show some vertical slices\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(im1[:,:,im1.shape[2]//2], cmap='Greys_r')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(im5[:,:,im5.shape[2]//2], cmap='Greys_r')\n",
    "plt.show()\n",
    "\n",
    "# Some vertical strain (being applied at the bottom) is visible!\n",
    "\n",
    "plt.hist(im1.ravel(), bins=256, range=[0, 65535])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGPBHKv3_iLW",
    "tags": []
   },
   "source": [
    "We see that the distribution of the greyvalues in the image has a peak at zero (mask of the outside of the sample), a gaussian-looking peak around 5000 which represents the air CT value and another gaussian peak at 30000 CT values which represents the most common CT value for the particles, although it's visible from the slices that there are some righter particles.\n",
    "\n",
    "\n",
    "# Digital Image/Volume Correlation for registration\n",
    "\n",
    "In this first section we will introduce Digital Volume Correlation in general, with the aim of mapping (\"registering\") a whole 3D volume into another, *i.e.*, to measure the overall deformation between the two volumes.\n",
    "\n",
    "3D correlation libraries are not so easy to find, here we will use *spam* (https://pypi.org/project/spam/) that includes this functionality. Spam proposes a non-rigid registration trying to find the optimal linear deformation function $\\Phi$ to map one image into the other.\n",
    "$\\Phi$ is expressed in homogeneous coordinates.\n",
    "\n",
    "## Homogeneous coordinates and $\\Phi$\n",
    "\n",
    "$\\Phi$ will be a 4x4 matrix as follows:\n",
    "\n",
    "<img width=\"30%\" src=\"https://zenodo.org/record/7140837/files/phi.png?download=1\"></a>\n",
    "                            \n",
    "With 3D coordinates for a point:\n",
    "\n",
    "$$ x = (\\text{x},y,z) $$\n",
    "\n",
    "we pad with a one and turn the coordinates into a column vector (this means we pass into a \"homogeneous\" coordinate system), to give:\n",
    "\n",
    "<img width=\"15%\" src=\"https://zenodo.org/record/7140837/files/x.png?download=1\"></a>\n",
    "\n",
    "We can then transform the coordinate with the deformation function $\\Phi$ as follows:\n",
    "\n",
    "$$ \\Phi.x = x' $$\n",
    "\n",
    "\n",
    "## Identity Matrix\n",
    "\n",
    "A $\\Phi$ which is the identity matrix does not deform the point\n",
    "\n",
    "<img width=\"40%\" src=\"https://zenodo.org/record/7140837/files/identity.png?download=1\"></a>\n",
    "\n",
    "\n",
    "### Translation\n",
    "\n",
    "A simple translation of the point x is applied with components\n",
    "\n",
    "$ t_x, t_y, t_z $\n",
    "\n",
    "<img width=\"30%\" src=\"https://zenodo.org/record/7140837/files/translation.png?download=1\"></a>\n",
    "\n",
    "When applied:\n",
    "\n",
    "<img width=\"30%\" src=\"https://zenodo.org/record/7140837/files/translation_applied.png?download=1\"></a>\n",
    "\n",
    "Remember that when dealing with differently binned images (see later), translations must be scaled, unlike linear transformations.\n",
    "\n",
    "\n",
    "### Linear transformations (rotations, \"zoom\", \"shear\")\n",
    "\n",
    "All remaining components of the transformation are found mixed together in the top left 3x3 matrix in $\\Phi$, leading to our familiar, from continuum mechanics, transformation gradient tensor **F** (see previous tutorial).\n",
    "\n",
    "For example uniaxial stretching can be performed by a change in the components along the diagonal of F:\n",
    "\n",
    "<img width=\"30%\" src=\"https://zenodo.org/record/7140837/files/zoom.png?download=1\"></a>\n",
    "\n",
    "Rotations in 3D can be quite complex.\n",
    "There are a number of different ways of representing rotations in 3D -- see this very clear explanation on Wikipedia (https://en.wikipedia.org/wiki/Rotation_formalisms_in_three_dimensions). A simple example of a rotation around the z axis of a an angle $\\theta$  would look like :\n",
    "\n",
    "<img width=\"30%\" src=\"https://zenodo.org/record/7140837/files/rotation.png?download=1\"></a>\n",
    "\n",
    "To represent rotations simply, we choose the *Rotation Vector* format, which is in the family of *axis-and-angle* representations.\n",
    "A rotation vector is a three-component vector whose direction is the rotation axis and whose length is the rotation angle in degrees.\n",
    "\n",
    "\n",
    "### Differences between images\n",
    "\n",
    "\n",
    "In image correlation we have two images -- *im1* and *im2* which differ in some way.\n",
    "We are looking for some $\\Phi$ that makes *im1* and *im2* as similar as possible.\n",
    "Here $x$ is a position vector:\n",
    "\n",
    "$$ \\text{im1}(x) = \\text{im2}(\\Phi.x) $$\n",
    "\n",
    "or:\n",
    "\n",
    "$$ \\text{im1}(x) - \\text{im2}(\\Phi.x) = 0 $$\n",
    "\n",
    "This is an important point to visualise: let's see what\n",
    "\n",
    "$$ \\text{im1}(x) - \\text{im2}(x) $$\n",
    "\n",
    "looks like for two different images :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "Wbd3sC4z_iLY",
    "outputId": "d832e5f3-eb58-4cd7-ea68-fcfa88414ec5"
   },
   "outputs": [],
   "source": [
    "diff = im1[:,:,im1.shape[2]//2] - im5[:,:,im5.shape[2]//2]\n",
    "plt.imshow(diff , cmap='coolwarm', vmin=-numpy.abs(diff).max(), vmax=numpy.abs(diff).max())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGCXmlVP_iLY"
   },
   "source": [
    "Blue means a negative number, grey = 0 and red is positive.\n",
    "\n",
    "\n",
    "We can see that there are differences between these images.\n",
    "However there is some alignment -- especially the top of the sample seems good, and there is increasing mis-overlap as we go downwards towards 300.\n",
    "This is relatively expected, since we are loading the sample from the bottom.\n",
    "The middles of some particles have low error -- but there are problems on the edges.\n",
    "Since the amount of dark and light voxels are approximately the same in the two images that have been subtracted, the sum of this image could be expected to be close to zero.\n",
    "\n",
    "Under this transformation, this difference is clearly not zero.\n",
    "If **im1** and **im2** have been acquired with a real measurement device which has some noise, even with the *a-priori* knowledge of Φ this difference will never be zero.\n",
    "In what follows, the objective will be to find the transformation Φ that minimises this difference as far as possible.\n",
    "\n",
    "If we'd like to define the difference between **im1** and **im2**, if we *square* the value of all the voxels in the difference, then low values will stay low, and highly negative and highly positive differences will both become highly positive.\n",
    "The choice of the *square* particular makes sense for Gaussian noise.\n",
    "\n",
    "$$ \\left(\\text{im1}(x) - \\text{im2}(x)\\right)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276
    },
    "id": "rL_8X0AM_iLf",
    "outputId": "4f2cb275-4a92-4de3-997c-e9addef9b3a4"
   },
   "outputs": [],
   "source": [
    "diff = im1[:,:,im1.shape[2]//2] - im5[:,:,im5.shape[2]//2]\n",
    "plt.imshow(diff**2)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_AL-ze6V_iLf"
   },
   "source": [
    "Here low values are well matched, and high values are poorly matched.\n",
    "\n",
    "\n",
    "### Image correlation -- Finding $\\Phi$\n",
    "\n",
    "We want to find $\\Phi$. Since it has 12 unknowns (4×4 = 16 components, of which the bottom row is known), this is quite a complicated problem.\n",
    "The first thing to do is to define an error *function* of Φ, which in this case is simply the classic *sum of squared difference* as introduced above, *i.e.,*\n",
    "\n",
    "$$ \\mathcal{T}(\\Phi) = \\frac{1}{2}\\sum_{x \\in ROI}{\\left(\\text{im1}(x) - \\text{im2}(\\Phi \\cdot x)\\right)^2} $$\n",
    "\n",
    "$\\mathcal{T}$ is a scalar function of the matrix $\\Phi$.\n",
    "The scalar returned is the sum-of-squares difference between **im1** and **im2** deformed by Φ.\n",
    "*ROI* is a 3D region-of-interest in which we are calculating this error, and for which we will now try to find $\\Phi$.\n",
    "Since we're trying to measure a $\\Phi$ in a small region, rather than for every voxel (which would be impossible), we have written a *weak* form of the solution.\n",
    "\n",
    "Given this error function we can state our objective more clearly: we would like to find a Φ that minimises this error function.\n",
    "In previous work we had searched the space of displacements, rotations, *etc.* making small steps (line searches) to decrease the error, but what follows is much faster and more robust:\n",
    "we have implemented the incremental <cite data-cite=\"lucas1981iterative\">(Lukas \\& Kanade, 1981)</cite> approach for image correlation.\n",
    "We use the formulation in homogeneous coordinates (*i.e.*, with $\\Phi$) by the LMT in <cite data-cite=\"tudisco2017extension\">(Tudisco et al., 2017)</cite>, build on previous work such as <cite data-cite=\"hild2012comparison\">(Hild \\& Roux, 2012)</cite>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Nqobcb4_iLf"
   },
   "source": [
    "Let's try to measure a single Phi with the above approach to map the whole on image 1 into image 5.\n",
    "\n",
    "Here we'll use the internal function programmed in spam, again, it is iterative, and will make progressive corrections to a guess initial $\\Phi$ function.\n",
    "The function will continue to iterate until the step size to correct Phi has become very small, or unless we hit a maximum number of iterations.\n",
    "\n",
    "Furthermore, to save time and increase robustness we will use a \"multiscale\" approach, which is a very common way to formuate problems in image analysis.\n",
    "In essence this means that we will downscale both images (8 times in this case) so that they are smaller, try to measure a $\\Phi$, then downscale the original images only 4 times, and try to refine the $\\Phi$, and repeat all the way back to the original images.\n",
    "\n",
    "This means that a rough fit (which is not confused by fine details) is initially computed quickly computed (since there are fewer data and also less noisy, given the inherent averaging in downscaling images).\n",
    "The multiscale process can be seen in the output below: please note that the final converged result at each scale is printed, with the current $\\Phi$ decomposed into **t** (a translation vector), **r** (a rotation vector in degrees), and **z** (a change of size of axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R8SrpG_D_iLg",
    "outputId": "ad18abc9-4d23-4e1a-b6d8-3351f0c380d6",
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import spam.DIC\n",
    "import spam.deformation\n",
    "\n",
    "registration = spam.DIC.registerMultiscale(im1, im5, 8, verbose=True, imShowProgress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h965W8dZl9tM",
    "outputId": "ae5000b4-0ec4-472d-ab48-c1091060c847"
   },
   "outputs": [],
   "source": [
    "registration['Phi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oTNuDx4J_iLh",
    "outputId": "f2b8dacf-d403-401d-a76f-281c148fc9c1"
   },
   "outputs": [],
   "source": [
    "# Decompose Phi from registration to make it more understandable\n",
    "PhiDecomposed = spam.deformation.decomposePhi(registration['Phi'])\n",
    "\n",
    "names = [['t', 'Translation px (z, y, x)'],\n",
    "         ['z', 'Axis stretch (z, y, x)'],\n",
    "         ['r', 'Rotation vector deg (z, y, x)']]\n",
    "\n",
    "for n, N in names:\n",
    "    print(f\"{N}:\\n\\t{PhiDecomposed[n]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIsqmEM3_iLh"
   },
   "source": [
    "This allow the compression in Z to be seen (axis stretch in Z less than 1) with radial expansion in Y and X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5NGlrH4_iLh"
   },
   "source": [
    "# Particle identification\n",
    "\n",
    "Particle-based analysis clearly requires the particles to be uniquely identified, which is really **the** key step in our opinion for the analysis of granular geomaterials.\n",
    "\n",
    "Here we will use a pretty standard technique to separate particles: first we will binarise the image into only black and white or true and false voxels (which will indicate the solid, granular phase *v.s.* everything else). Given the low noise in the image we can quite safely just apply a threshold CT value, around 20000.\n",
    "\n",
    "Thereafter a 3D watershed algorithm will be applied. This was invented by <cite data-cite=\"beucher1979use\">(Beucher, 1979)</cite>, and improved by <cite data-cite=\"beucher2018morphological\">(Beucher \\& Meyer, 2018)</cite>.\n",
    "Here we're using the helper function from the *spam* library <cite data-cite=\"stamati2020spam\">(Stamati et al, 2020)</cite> which implement's ITK's watershed <cite data-cite=\"beare2006watershed\">(Beare et al., 2006)</cite>.\n",
    "In short, in this sub-species of this method, we will first attempt to find \"markers\" in the inside of each particle in 3D.\n",
    "This is achieved by working on the binary image - the 3D Euclidean distance map is computed on the grain phase, computing the distance for each grain-voxel to the other voxels.\n",
    "This field is expected to increase towards the centre of grains, and in the case of quite rounded particles is expected to have a single peak in the middle of the particle.\n",
    "Therefore the local maxima of the distance map are identified -- in order to have some robustness to convex but non-spherical shapes some nearby maxima can be merged into a single one.\n",
    "\n",
    "Once this is done, each merged marker will be given a unique number (label), and these markers will be grown using the watershed algorithm and the Euclidean distance map until they cover all the solid phase and generally should only touch at the contacts.\n",
    "\n",
    "There are subtle differences between \"interpixel\" algorithms and \"watershed line\" implementations: the interpixel will label all voxels of the grain phase as belonging to a label, whereas the watershed line will split particles at the contacts.\n",
    "The algorithm used here is an interpixel one, so we'll get touching labels out directly.\n",
    "\n",
    "Let's run the algorithm and see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "aYea8m7f_iLh",
    "outputId": "db283c09-9e78-4876-e315-a397e3774a56"
   },
   "outputs": [],
   "source": [
    "import spam.label\n",
    "import os.path\n",
    "import tifffile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if os.path.isfile(data_path / \"M2EA05-01-bin4-lab.tif\"):\n",
    "  print(\"Reading existing file!\")\n",
    "  im1lab = tifffile.imread(data_path / \"M2EA05-01-bin4-lab.tif\")\n",
    "else:\n",
    "  # This could take a few minutes...\n",
    "  im1lab = spam.label.watershed(im1 > 20000)\n",
    "\n",
    "# Let's see what happened\n",
    "print(f\"The maximum value found in the labelled im1 is {im1lab.max()}\")\n",
    "\n",
    "print(im1lab.max())\n",
    "\n",
    "# Let's show some vertical slices\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Im1 Labelled image\")\n",
    "plt.imshow(im1lab[:,:,im1.shape[2]//2],\n",
    "           interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Im1 Labelled image\")\n",
    "plt.imshow(im1lab[:,:,im5.shape[2]//2],\n",
    "           cmap=spam.label.randomCmap,\n",
    "           interpolation='none')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zjK6h3i_iLh"
   },
   "source": [
    "How to interpret these images? In general labelling algorithms proceed down one direction, as can be seen no the left, label values seem to increase as Z increases.\n",
    "\n",
    "On the right, we're using a special colourmap inspired by <cite data-cite=\"glasbey2007colour\">(Glasbey et al, 2007)</cite> where each level of the colourmap is randomised to accentuate differences between nearby values.\n",
    "\n",
    "What is visible here is that each grain pixel (identified previously with a greyscale threshold) has a label number; the non-grain phase has value 0 and each identified contiguous (i.e., touching) blob of voxels has a unique label number identifying it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQlbdgYa_iLi"
   },
   "source": [
    "Generally the algorithm seems to have done a good job: there is a possible merged label in the middle of the volume (this is called undersegmentation), however oversegmentation (a real particle split into two or more labels) does not seem to be present.\n",
    "\n",
    "A labelled image is a allows discrete analysis, but extracting and interrogating each label individually.\n",
    "This is very easy to program, it simply means fetching all the voxels corresponding to a given label: thereafter a number of computations are easy, such as:\n",
    "\n",
    " - the barycentre of the object (the spatial average of the coordinates of each voxel)\n",
    " - the volume (the sum of the voxels)\n",
    " - Some shape/orientation descriptors such as the orientated caliper lengths or \"feret diameters\" or the moment of intertia\n",
    " \n",
    "Other measurements on discretely voxelsied objects require a little more care:\n",
    "\n",
    " - surface area\n",
    " - interparticle contact presence and orientation\n",
    " \n",
    "The article <cite data-cite=\"ando2018ease\">(Andò \\& Viggiani 2018)</cite> treats this exact problem in more detail.\n",
    "\n",
    "As an illustration, the volume of each particle can be easily computed, allowing the diameter of the sphere of the same volume to be computed, from here a per-particle, particle size distribution can be computed.\n",
    "Please note that the *2*15.56 represents the conversion between pixels and mm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "id": "diSvYISE_iLi",
    "outputId": "cf084410-62ea-4e73-bdaa-6f018e21f0a8"
   },
   "outputs": [],
   "source": [
    "im1radii = spam.label.equivalentRadii(im1lab)\n",
    "\n",
    "import spam.plotting\n",
    "spam.plotting.plotParticleSizeDistribution(im1radii*2*15.56/1000,\n",
    "                                           cumulative=True,\n",
    "                                           cumulativePassing=True,\n",
    "                                           mode=\"mass\",\n",
    "                                           logScaleX=True,\n",
    "                                           logScaleY=False,\n",
    "                                           units=\"mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPhUc39k_iLi"
   },
   "source": [
    "You might have seen pictures where grains are coloured according to some scalar.\n",
    "There are programmatically elegant ways to do this, but we can use a brute force approach here where all label values are simply replaced in a new image with corresponding values in a vector... for example the equivalnet radii would work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 271
    },
    "id": "AsprXc_r_iLj",
    "outputId": "800a6e5d-6237-45e7-8ebe-8355e025e8c0"
   },
   "outputs": [],
   "source": [
    "tmp = spam.label.convertLabelToFloat(im1lab[:, :, (im1.shape[2]//2)-2:(im1.shape[2]//2)+2],\n",
    "                                     im1radii*2*15.56/1000)\n",
    "\n",
    "plt.title(\"Particle equivalent radii in mm\")\n",
    "plt.imshow(tmp[:,:,2],\n",
    "           interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahTNvSrs_iLj"
   },
   "source": [
    "We can also use such measurements to make a quality control of the specimen... for example checking the spatial distribution of the large particles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LisPmyR_iLj"
   },
   "source": [
    "# Particle Tracking\n",
    "\n",
    "For a particle tracking approach we need to identify objects in the other image.\n",
    "It seems reasonable to keep the threshold the same, so let's compute im5label, and compare it to im1label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "PUTPJqBt_iLj",
    "outputId": "0b9e7409-ac9a-4ec4-fb14-d9b55e009101"
   },
   "outputs": [],
   "source": [
    "im5lab = spam.label.watershed(im5 > 20000)\n",
    "print(f\"The maximum value found in the labelled im1 is {im1lab.max()}\")\n",
    "print(f\"The maximum value found in the labelled im5 is {im5lab.max()}\")\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Im1 labelled image\")\n",
    "plt.imshow(im1lab[:,:,im1.shape[2]//2],\n",
    "           cmap=spam.label.randomCmap,\n",
    "           interpolation='none')\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Im5 labelled image\")\n",
    "plt.imshow(im5lab[:,:,im5.shape[2]//2],\n",
    "           cmap=spam.label.randomCmap,\n",
    "           interpolation='none')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbAUbsr__iLj"
   },
   "source": [
    "Looking at label colours at the top (Z=low) of the volume, particle seem to have corresponding colours,\n",
    "however this correspondence is lost towards the bottom of the volume.\n",
    "This is either due to different amount of over- and under-segmentation in both images, which will make tracking difficult, or more benignly due to particle rearrangements (especially in the Z-direction) which will chang ethe sequencing of labels.\n",
    "\n",
    "A particle tracking technique siply now becomes a question of linking labels in image 1 to labels in image 5.\n",
    "This tracking/particle recognition can become quite sophisticated, but here we will just match particles based on nearest neighbours.\n",
    "More sophisticated techniques such as ID-Track have been developed for geomaterials <cite data-cite=\"ando2012grain\">(Andò et al., 2012)</cite>, but the generaly framework worth delving into is the  \"Hungarian Method\" <cite data-cite=\"kuhn1955hungarian\">(Kuhn, 1955)</cite>.\n",
    "\n",
    "This means that we will compute particle centers in image 1, apply the above-measured Phi to each centre to move it to an approximate position in image 5, and match each image 1 particle to the closest in image 5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fFqO2b5R_iLj",
    "outputId": "1e08c50d-8350-46b5-e993-b1849b6c09ed"
   },
   "outputs": [],
   "source": [
    "im1centers = spam.label.centresOfMass(im1lab)\n",
    "# Each row in im1centers represents the z, y, x positions in pixels of the particle of that row number.\n",
    "print(im1centers)\n",
    "im5centers = spam.label.centresOfMass(im5lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "qKi1sWnc_iLk",
    "outputId": "aee1e50f-07d1-4339-a872-c9862e072967",
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "registrationDisplacements = spam.DIC.applyRegistrationToPoints(registration['Phi'],\n",
    "                                                        (numpy.array(im1.shape) - 1) / 2.0,\n",
    "                                                        im1centers,\n",
    "                                                        applyF='no',\n",
    "                                                        verbose=True)[:, 0:3, -1]\n",
    "\n",
    "im1centersDeformed = im1centers + registrationDisplacements\n",
    "\n",
    "ax = plt.figure().add_subplot(projection='3d')\n",
    "ax.quiver(im1centers[:,2],\n",
    "          im1centers[:,1],\n",
    "          im1centers[:,0],\n",
    "          registrationDisplacements[:,2],\n",
    "          registrationDisplacements[:,1],\n",
    "          registrationDisplacements[:,0],\n",
    "          length=1,\n",
    "          normalize=False)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OnnGB1vT_iLk"
   },
   "outputs": [],
   "source": [
    "import scipy.spatial\n",
    "\n",
    "distanceMatrix = scipy.spatial.distance_matrix(im1centersDeformed, im5centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8NQ2yEzX_iLk"
   },
   "outputs": [],
   "source": [
    "labelCorrespondence5_to_1 = numpy.zeros(distanceMatrix.shape[1])\n",
    "\n",
    "for im5l in range(distanceMatrix.shape[1]):\n",
    "    #print(numpy.argmin(distanceMatrix[im1l]))\n",
    "    labelCorrespondence5_to_1[im5l] = numpy.argmin(distanceMatrix[:,im5l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262
    },
    "id": "ROxR0TYy_iLk",
    "outputId": "7b2d1638-a158-4d86-c4fa-954df6af7523"
   },
   "outputs": [],
   "source": [
    "im5relab = spam.label.convertLabelToFloat(im5lab, labelCorrespondence5_to_1)\n",
    "im5newcenters = spam.label.centresOfMass(im5relab)\n",
    "\n",
    "# Let's show some vertical slices\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(im1lab[:,:,im1.shape[2]//2],\n",
    "           cmap=spam.label.randomCmap, interpolation='none')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(im5relab[:,:,im5relab.shape[2]//2],\n",
    "           cmap=spam.label.randomCmap, interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "id": "rVjO-yhf_iLk",
    "outputId": "db650a34-268f-4a2e-ef8e-4c46c89c2f34"
   },
   "outputs": [],
   "source": [
    "particleTrackingDisplacements = im1centers - im5newcenters\n",
    "\n",
    "ax = plt.figure().add_subplot(projection='3d')\n",
    "ax.quiver(im1centers[:,2],\n",
    "          im1centers[:,1],\n",
    "          im1centers[:,0],\n",
    "          particleTrackingDisplacements[:,2],\n",
    "          particleTrackingDisplacements[:,1],\n",
    "          particleTrackingDisplacements[:,0],\n",
    "          length=1,\n",
    "          normalize=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262
    },
    "id": "ww2MQSJX_iLk",
    "outputId": "fb4660cf-504c-406d-9b83-5010b3ca5f14"
   },
   "outputs": [],
   "source": [
    "tmp = spam.label.convertLabelToFloat(im1lab[:, :, (im1.shape[2]//2)-2:(im1.shape[2]//2)+2],\n",
    "                               numpy.sqrt(numpy.sum(numpy.square(particleTrackingDisplacements), axis=1)))\n",
    "plt.imshow(tmp[:,:,2],\n",
    "           interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWjScq8c_iLk"
   },
   "source": [
    "# Discrete DVC\n",
    "\n",
    "Now we will use **only** the labelled image in the reference configuration (im1lab) and adapt the earlier registration procedure to operate on a grain-by-grain basis.\n",
    "In the language introduced above, we will perform one registration procedure per grain setting the ROI to be the label for each grain.\n",
    "This \"Discrete DVC\" procedure was proposed in <cite data-cite=\"hall2009strain\">(Hall et al., 2009)</cite> and has found a lot of success in image-based particle tracking.\n",
    "\n",
    "It has the advantage that the labelled image is only used once and thus can be optimised and manually checked and corrected until it is almost as good as possible.\n",
    "\n",
    "As before we will take into account the overall registration between image 1 and image 5 as the intial guess for particle displacements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xfJvpCi__iLl",
    "outputId": "6c844f28-89b9-4157-e162-45bd300a355e"
   },
   "outputs": [],
   "source": [
    "import spam.helpers\n",
    "tifffile.imsave(data_path / \"M2EA05-01-bin4-lab.tif\", im1lab)\n",
    "print(f\"saved \", data_path / \"M2EA05-01-bin4-lab.tif\")\n",
    "spam.helpers.writeRegistrationTSV(data_path / \"M2EA05-01-05-bin4.tsv\",\n",
    "                                    (numpy.array(im1.shape) - 1) / 2.0,\n",
    "                                    registration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! spam-ddic \"../data/M2EA05-01-bin4.tif\" \"../data/M2EA05-01-bin4-lab.tif\" \"../data/M2EA05-05-bin4.tif\" -pf \"../data/M2EA05-01-05-bin4.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "BPZc4p0f_iLl",
    "outputId": "2d7ec962-fecd-4bd0-da6f-23e7fbdc1dd3",
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "DDIC = spam.helpers.readCorrelationTSV(data_path / \"M2EA05-01-bin4-M2EA05-05-bin4-ddic.tsv\")['PhiField']\n",
    "\n",
    "tmp = spam.label.convertLabelToFloat(im1lab[:, :, (im1.shape[2]//2)-2:(im1.shape[2]//2)+2],\n",
    "                                     numpy.sqrt(numpy.sum(numpy.square(DDIC[:,0:3,-1]), axis=1)))\n",
    "plt.imshow(tmp[:,:,2],\n",
    "           interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CndGAwAa_iLl"
   },
   "source": [
    "## Computing strains\n",
    "\n",
    "What to do with this 3D displacement field? A natural question is to try to convert it into strains.\n",
    "\n",
    "This is essentially a coarse-graining problem (mixed with a strain calculation problem).\n",
    "The two main families of approaches are:\n",
    "\n",
    "  - Grain based: particle centres are triangulated with an appropriate algorithm (Delaunay-style), making tetrahedra in 3D thus creating a basis on which strains can be calculated\n",
    "  - Spatial: A 3D grid is computed and particle kinematics are computed on the grid, where cubic shape functions can be used to compute strains.\n",
    "  \n",
    "Both options are available in spam, and both will be illustrated now (although the tetrahedra; strains will be rendered with an external programme -- Paraview).\n",
    "\n",
    "### Tetrahedra\n",
    "\n",
    "In the case of dense, mono-disperse spherical packings a Delaunay triangulation will create tetrahedra whose vertices will be at the particle contacts. In the case of polydispersity, this is not respected, so adding a weight to the triangulation (making it a Radical Delaunay, or \"Laguerre\" triangulation) will help improve things.\n",
    "\n",
    "Strains can be computed on this tetrahedral basis as per <cite data-cite=\"bagi1996stress\">(Bagi, 1996)</cite> and later <cite data-cite=\"zhang2015large\">(Zhang \\& Regueiro, 2015)</cite>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TeHKV_dR_iLl",
    "outputId": "8013640b-a086-4cde-e8ac-f1c0075dbf67"
   },
   "outputs": [],
   "source": [
    "! spam-discreteStrain \"../data/M2EA05-01-bin4-M2EA05-05-bin4-ddic.tsv\" -tri -a 300 -rl \"../data/M2EA05-01-bin4-lab.tif\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bp_1RzYl_iLl",
    "width": 0.5
   },
   "source": [
    "This file is then loaded into paraview, showing a mesh of tetrahedra that link particle centres.\n",
    "For this visualisation, the cylinder is cut in half to reveal tetrahedra, which are coloured either by the devatoric strains computed by the displacements of the four particles at each vertex, or volumetric strains:\n",
    "\n",
    "<img width=\"50%\" src=\"https://zenodo.org/record/7142405/files/meshStrains.jpg?download=1\"></a>\n",
    "\n",
    "This clearly shows some clear concentrations of deviatoric strain early in the test.\n",
    "We can compare these fields to the grid processing method:\n",
    "\n",
    "## Grid analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_x6MZJ0g_iLl",
    "outputId": "a30fd7e3-863a-42c8-cb65-f1e78b073423"
   },
   "outputs": [],
   "source": [
    "! spam-passPhiField -im1 \"../data/M2EA05-01-bin4-lab.tif\" -ns 10 -vtk -pf \"../data/M2EA05-01-bin4-M2EA05-05-bin4-ddic.tsv\" -tif\n",
    "! spam-regularStrain \"../data/M2EA05-01-bin4-M2EA05-05-bin4-ddic-passed-ns10.tsv\" -Q8 -rst 1 -tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "Q9vrCcvC_iLl",
    "outputId": "ba204463-0a27-41ff-edb9-403ebda5ad41"
   },
   "outputs": [],
   "source": [
    "plt.subplot(1,3,1)\n",
    "zField = tifffile.imread(data_path / \"M2EA05-01-bin4-M2EA05-05-bin4-ddic-passed-ns10-Zdisp.tif\")\n",
    "plt.title(\"Z displacment field on grid\")\n",
    "plt.imshow(zField[:,zField.shape[1]//2], cmap='inferno')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "devField = tifffile.imread(data_path / \"M2EA05-01-bin4-M2EA05-05-bin4-ddic-passed-ns10-dev-Q8.tif\")\n",
    "plt.title(\"Deviatoric strain field on grid\")\n",
    "plt.imshow(devField[:,devField.shape[1]//2], cmap='inferno', vmin=0, vmax=0.1)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "volField = tifffile.imread(data_path / \"M2EA05-01-bin4-M2EA05-05-bin4-ddic-passed-ns10-vol-Q8.tif\")\n",
    "plt.title(\"Volumetric strain field on grid\")\n",
    "plt.imshow(volField[:,volField.shape[1]//2], cmap='coolwarm', vmin=-0.1, vmax=0.1)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAWkBs2__iLm"
   },
   "source": [
    "It is our contention that these continous fields are easier to interpret.\n",
    "A final question may come to mind: why not simply split the reference image into cubic correlation volumes (and not use the labelled image), since the displacement field is interpolated onto a regular grid using this method?\n",
    "The fact of the matter is that the labelled image is the perfect definition of the objects that will move in this image, and so i the most pertinent base on which to do the tracking!"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "institution": "EPFL Center for Imaging",
    "name": "Edward Andò"
   }
  ],
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "image-analysis-field-guide",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "title": "3D Analysis"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
